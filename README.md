# MULQA: Adapting Multimodal Models to Unimodal Tasks by Ensembling FLAVA with ALBERT

Team members:

Akash Gujju

Anushka Kamath

Trisha Mandal

Varsha Kini

Abstract:
In this paper, we investigate the performance of the FLAVA model, a powerful vision and language alignment model, and its ensembled version with ALBERT, called MULQA, on unimodal tasks. While FLAVA is designed primarily for multimodal tasks such as visual question answering (VQA) and image captioning, evaluating its performance on text-only tasks is essential to assess language and context understanding capabilities. We compare the traditional FLAVA and MULQA model on several VQA text-only tasks, which serve as benchmarks for evaluating the models' performance in understanding language without visual input. Our analysis of the models' performance on these tasks can provide insights into the effectiveness of multimodal models in unimodal tasks and guide future research in developing more accurate and versatile models capable of handling a wide range of linguistic features and contexts.

We worked on top of the baseline found in public domain namely

FLAVA(https://github.com/facebookresearch/multimodal/tree/main/examples/flava)
