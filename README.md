# MULQA: Adapting Multimodal Models to Unimodal Tasks by Ensembling FLAVA with ALBERT

## Team members: 

1.Akash Gujju
2.Anushka Kamath
3.Trisha Mandal
4.Varsha Kini

## Abstract:
In this paper, we investigate the performance of the FLAVA model, a powerful vision and language alignment model, and its ensembled version with ALBERT, called MULQA, on unimodal tasks. While FLAVA is designed primarily for multimodal tasks such as visual question answering (VQA) and image captioning, evaluating its performance on text-only tasks is essential to assess language and context understanding capabilities. We compare the traditional FLAVA and MULQA model on several VQA text-only tasks, which serve as benchmarks for evaluating the models' performance in understanding language without visual input. Our analysis of the models' performance on these tasks can provide insights into the effectiveness of multimodal models in unimodal tasks and guide future research in developing more accurate and versatile models capable of handling a wide range of linguistic features and contexts.

We worked on top of the baseline found in public domain namely

FLAVA(https://github.com/facebookresearch/multimodal/tree/main/examples/flava)
